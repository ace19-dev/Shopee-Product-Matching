{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.10\n",
      "Sat Apr 10 00:54:20 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 455.23.05    Driver Version: 455.23.05    CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  TITAN V             On   | 00000000:19:00.0 Off |                  N/A |\n",
      "| 29%   43C    P8    24W / 250W |      1MiB / 12066MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  TITAN V             On   | 00000000:1A:00.0 Off |                  N/A |\n",
      "| 37%   51C    P8    28W / 250W |      1MiB / 12066MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  TITAN V             On   | 00000000:67:00.0 Off |                  N/A |\n",
      "| 40%   54C    P8    31W / 250W |      1MiB / 12066MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  TITAN V             On   | 00000000:68:00.0 Off |                  N/A |\n",
      "| 38%   52C    P8    28W / 250W |     19MiB / 12065MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2353      G   /usr/lib/xorg/Xorg                  0MiB |\n",
      "|    1   N/A  N/A      2353      G   /usr/lib/xorg/Xorg                  0MiB |\n",
      "|    2   N/A  N/A      2353      G   /usr/lib/xorg/Xorg                  0MiB |\n",
      "|    3   N/A  N/A      2353      G   /usr/lib/xorg/Xorg                 17MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2019 NVIDIA Corporation\n",
      "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
      "Cuda compilation tools, release 10.1, V10.1.243\n",
      "Ubuntu 18.04.5 LTS \\n \\l\n",
      "\n",
      "1.7.1+cu101\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "\n",
    "# cuda version\n",
    "!nvidia-smi\n",
    "!nvcc --version\n",
    "\n",
    "# ubuntu version\n",
    "!cat /etc/issue\n",
    "\n",
    "# pytorch version\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "# # tensorflow version\n",
    "# import tensorflow as tf\n",
    "# print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /home/ace19/my-repo/Shopee-Product-Matching/input/my-libs/timm-0.4.5-py3-none-any.whl\n",
      "Collecting torchvision (from timm==0.4.5)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "  Downloading https://files.pythonhosted.org/packages/93/8a/82062a33b5eb7f696bf23f8ccf04bf6fc81d1a4972740fb21c2569ada0a6/torchvision-0.9.1-cp37-cp37m-manylinux1_x86_64.whl (17.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 17.4MB 108kB/s eta 0:00:01    51% |████████████████▋               | 9.0MB 12.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch>=1.4 (from timm==0.4.5)\n",
      "  Downloading https://files.pythonhosted.org/packages/56/74/6fc9dee50f7c93d6b7d9644554bdc9692f3023fa5d1de779666e6bf8ae76/torch-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (804.1MB)\n",
      "\u001b[K    15% |�^C██                           | 128.0MB 12.3MB/s eta 0:00:55 0% |                                | 593kB 10.6MB/s eta 0:01:16    3% |█▏                              | 29.8MB 9.8MB/s eta 0:01:20    6% |██                              | 51.0MB 10.7MB/s eta 0:01:11    7% |██▌                             | 62.8MB 11.1MB/s eta 0:01:07    8% |██▋                             | 65.8MB 11.6MB/s eta 0:01:04    9% |███                             | 73.6MB 12.2MB/s eta 0:01:01    9% |███                             | 74.2MB 11.2MB/s eta 0:01:06    9% |███                             | 77.2MB 12.2MB/s eta 0:01:00    15% |█████                           | 125.5MB 9.1MB/s eta 0:01:15\n",
      "\n",
      "\u001b[31mOperation cancelled by user\u001b[0m\n",
      "\u001b[?25h\u001b[33mRequirement '../input/my-libs/cupy_cuda102-8.6.0-cp37-cp37m-manylinux1_x86_64.whl' looks like a filename, but the file does not exist\u001b[0m\n",
      "Processing /home/ace19/my-repo/Shopee-Product-Matching/input/my-libs/cupy_cuda102-8.6.0-cp37-cp37m-manylinux1_x86_64.whl\n",
      "\u001b[31mException:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3/dist-packages/pip/basecommand.py\", line 215, in main\n",
      "    status = self.run(options, args)\n",
      "  File \"/usr/lib/python3/dist-packages/pip/commands/install.py\", line 353, in run\n",
      "    wb.build(autobuilding=True)\n",
      "  File \"/usr/lib/python3/dist-packages/pip/wheel.py\", line 749, in build\n",
      "    self.requirement_set.prepare_files(self.finder)\n",
      "  File \"/usr/lib/python3/dist-packages/pip/req/req_set.py\", line 380, in prepare_files\n",
      "    ignore_dependencies=self.ignore_dependencies))\n",
      "  File \"/usr/lib/python3/dist-packages/pip/req/req_set.py\", line 620, in _prepare_file\n",
      "    session=self.session, hashes=hashes)\n",
      "  File \"/usr/lib/python3/dist-packages/pip/download.py\", line 809, in unpack_url\n",
      "    unpack_file_url(link, location, download_dir, hashes=hashes)\n",
      "  File \"/usr/lib/python3/dist-packages/pip/download.py\", line 715, in unpack_file_url\n",
      "    unpack_file(from_path, location, content_type, link)\n",
      "  File \"/usr/lib/python3/dist-packages/pip/utils/__init__.py\", line 617, in unpack_file\n",
      "    flatten=not filename.endswith('.whl')\n",
      "  File \"/usr/lib/python3/dist-packages/pip/utils/__init__.py\", line 500, in unzip_file\n",
      "    zipfp = open(filename, 'rb')\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/ace19/my-repo/Shopee-Product-Matching/input/my-libs/cupy_cuda102-8.6.0-cp37-cp37m-manylinux1_x86_64.whl'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# # !pip install ../input/my-libs/timm-0.3.4-py3-none-any.whl\n",
    "# !pip install ../input/my-libs/timm-0.4.5-py3-none-any.whl\n",
    "# # !pip install ../input/my-libs/Pillow-8.0.1-cp37-cp37m-manylinux1_x86_64.whl\n",
    "# # !pip install ../input/my-libs/requests-2.25.0-py2.py3-none-any.whl\n",
    "# !pip install ../input/my-libs/cupy_cuda102-8.6.0-cp37-cp37m-manylinux1_x86_64.whl\n",
    "# # !pip install ../input/my-libs/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\n",
    "# # !pip install ../input/my-libs/transformers-4.4.2-py3-none-any.whl\n",
    "\n",
    "# # !pip install ../input/my-libs/Keras_Applications-1.0.8-py3-none-any.whl\n",
    "# # !pip install ../input/my-libs/efficientnet-1.1.0-py3-none-any.whl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: cannot create regular file '/opt/conda/envs/rapids.tar.gz': Permission denied\n",
      "tar (child): rapids.tar.gz: Cannot open: No such file or directory\n",
      "tar (child): Error is not recoverable: exiting now\n",
      "tar: Child returned status 2\n",
      "tar: Error is not recoverable: exiting now\n",
      "cp: cannot stat '/opt/conda/envs/rapids/lib/libxgboost.so': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/cdeotte/rapids\n",
    "\n",
    "import sys\n",
    "!cp ../input/rapids/rapids.0.18.0 /opt/conda/envs/rapids.tar.gz\n",
    "!cd /opt/conda/envs/ && tar -xzvf rapids.tar.gz > /dev/null\n",
    "sys.path = [\"/opt/conda/envs/rapids/lib/python3.7/site-packages\"] + sys.path\n",
    "sys.path = [\"/opt/conda/envs/rapids/lib/python3.7\"] + sys.path\n",
    "sys.path = [\"/opt/conda/envs/rapids/lib\"] + sys.path \n",
    "!cp /opt/conda/envs/rapids/lib/libxgboost.so /opt/conda/lib/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, gc\n",
    "import cv2, matplotlib.pyplot as plt\n",
    "import cudf, cuml, cupy\n",
    "from cuml.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from cuml.neighbors import NearestNeighbors\n",
    "\n",
    "print('RAPIDS',cuml.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/shopee-product-matching/train.csv')\n",
    "tmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\n",
    "train['target'] = train.label_group.map(tmp)\n",
    "print('train shape is', train.shape )\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/shopee-product-matching/test.csv')\n",
    "test_gf = cudf.read_csv('../input/shopee-product-matching/test.csv')\n",
    "print('Test shape is', test_gf.shape )\n",
    "test_gf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Text Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def removePunctuation(text):\n",
    "    punc_translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    return text.translate(punc_translator)\n",
    "\n",
    "\n",
    "test['title_clean'] = test['title'].apply(removePunctuation)\n",
    "title_to_use = cudf.DataFrame(test).title_clean\n",
    "\n",
    "print('Computing text embeddings...')\n",
    "tfidf_vec = TfidfVectorizer(stop_words='english', \n",
    "                            binary=True, \n",
    "                            lowercase=True,\n",
    "#                             max_df = 0.5,\n",
    "#                             min_df = 2,\n",
    "                            max_features=25000)\n",
    "text_embeddings = tfidf_vec.fit_transform(title_to_use).toarray().astype(np.float32)\n",
    "# text_embeddings = tfidf_vec.fit_transform(title_to_use)\n",
    "print('text embeddings shape',text_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fix\n",
    "\n",
    "# from cudf import Series\n",
    "# from cuml.common.sparsefuncs import csr_row_normalize_l2\n",
    "\n",
    "\n",
    "# # preds = []\n",
    "\n",
    "# def efficient_csr_cosine_similarity(query, tfidf_matrix, matrix_normalized=False):\n",
    "#     query = csr_row_normalize_l2(query, inplace=False)\n",
    "#     if not matrix_normalized:\n",
    "#         tfidf_matrix = csr_row_normalize_l2(tfidf_matrix, inplace=False)\n",
    "    \n",
    "#     return tfidf_matrix.dot(query.T)\n",
    "\n",
    "\n",
    "# def document_search(text_df, query, vectorizer, tfidf_matrix, top_n=3):\n",
    "#     query_vec = vectorizer.transform(query)\n",
    "#     similarities = efficient_csr_cosine_similarity(query_vec, tfidf_matrix, matrix_normalized=True)\n",
    "#     similarities = similarities.todense().reshape(-1)\n",
    "#     best_idx = similarities.argsort()[-top_n:][::-1]\n",
    "#     o = test.iloc[cupy.asnumpy(best_idx)].posting_id.values\n",
    "    \n",
    "#     return o.tolist()\n",
    "    \n",
    "# #     pp = cudf.DataFrame({\n",
    "# #         'text': text_df['text'].iloc[best_idx],\n",
    "# #         'similarity': similarities[best_idx]\n",
    "# #     })\n",
    "# #     return pp\n",
    "\n",
    "# _knn = 50\n",
    "# if text_embeddings.shape[0] <= 3:\n",
    "#     _knn = 3\n",
    "# else:\n",
    "#     _knn = 50\n",
    "# preds = document_search(test, title_to_use, tfidf_vec, text_embeddings, top_n=_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "CHUNK = 1024\n",
    "\n",
    "print('Finding similar titles...')\n",
    "CTS = len(test) // CHUNK\n",
    "if len(test) % CHUNK != 0: \n",
    "    CTS += 1\n",
    "\n",
    "for j in range( CTS ):\n",
    "    \n",
    "    a = j*CHUNK\n",
    "    b = (j+1)*CHUNK\n",
    "    b = min(b,len(test))\n",
    "    print('chunk',a,'to',b)\n",
    "    \n",
    "    # COSINE SIMILARITY DISTANCE\n",
    "    cts = cupy.matmul(text_embeddings, text_embeddings[a:b].T).T\n",
    "    \n",
    "    for k in range(b-a):\n",
    "        print('cts[k,]: ', cts[k,])\n",
    "        IDX = cupy.where(cts[k,]>0.7)[0]\n",
    "        print('IDX: ', IDX)\n",
    "        o = test.iloc[cupy.asnumpy(IDX)].posting_id.values\n",
    "        preds.append(o.tolist())\n",
    "        \n",
    "del tfidf_vec, text_embeddings\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if len(test) > 3:\n",
    "#     KNN = 50\n",
    "# else : \n",
    "#     KNN = 3\n",
    "\n",
    "# model = NearestNeighbors(n_neighbors = KNN)\n",
    "# model.fit(text_embeddings)\n",
    "# distances, indices = model.kneighbors(text_embeddings)\n",
    "# print('distances: ', distances)\n",
    "# print('indices: ', indices)\n",
    "\n",
    "# preds = []\n",
    "# for k in tqdm(range(text_embeddings.shape[0])):\n",
    "#     idx = np.where(distances[k,] < 0.8)[0]\n",
    "#     ids = indices[k,idx]\n",
    "#     posting_ids = test['posting_id'].iloc[ids].values\n",
    "#     preds.append(posting_ids)\n",
    "\n",
    "# del model, distances, indices\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text_embeddings'] = preds\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Text Embeddings2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# import tensorflow as tf\n",
    "# import efficientnet.tfkeras as efn\n",
    "# from tqdm.notebook import tqdm\n",
    "# from cuml.neighbors import NearestNeighbors\n",
    "# from shutil import copyfile\n",
    "# copyfile(src = \"../input/shopee-external-models/tokenization.py\", dst = \"../working/tokenization.py\")\n",
    "# import tokenization\n",
    "# import tensorflow_hub as hub\n",
    "\n",
    "\n",
    "# # Arcmarginproduct class keras layer\n",
    "# class ArcMarginProduct(tf.keras.layers.Layer):\n",
    "#     '''\n",
    "#     Implements large margin arc distance.\n",
    "\n",
    "#     Reference:\n",
    "#         https://arxiv.org/pdf/1801.07698.pdf\n",
    "#         https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n",
    "#             blob/master/src/modeling/metric_learning.py\n",
    "#     '''\n",
    "#     def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n",
    "#                  ls_eps=0.0, **kwargs):\n",
    "\n",
    "#         super(ArcMarginProduct, self).__init__(**kwargs)\n",
    "\n",
    "#         self.n_classes = n_classes\n",
    "#         self.s = s\n",
    "#         self.m = m\n",
    "#         self.ls_eps = ls_eps\n",
    "#         self.easy_margin = easy_margin\n",
    "#         self.cos_m = tf.math.cos(m)\n",
    "#         self.sin_m = tf.math.sin(m)\n",
    "#         self.th = tf.math.cos(math.pi - m)\n",
    "#         self.mm = tf.math.sin(math.pi - m) * m\n",
    "\n",
    "#     def get_config(self):\n",
    "\n",
    "#         config = super().get_config().copy()\n",
    "#         config.update({\n",
    "#             'n_classes': self.n_classes,\n",
    "#             's': self.s,\n",
    "#             'm': self.m,\n",
    "#             'ls_eps': self.ls_eps,\n",
    "#             'easy_margin': self.easy_margin,\n",
    "#         })\n",
    "#         return config\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         super(ArcMarginProduct, self).build(input_shape[0])\n",
    "\n",
    "#         self.W = self.add_weight(\n",
    "#             name='W',\n",
    "#             shape=(int(input_shape[0][-1]), self.n_classes),\n",
    "#             initializer='glorot_uniform',\n",
    "#             dtype='float32',\n",
    "#             trainable=True,\n",
    "#             regularizer=None)\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         X, y = inputs\n",
    "#         y = tf.cast(y, dtype=tf.int32)\n",
    "#         cosine = tf.matmul(\n",
    "#             tf.math.l2_normalize(X, axis=1),\n",
    "#             tf.math.l2_normalize(self.W, axis=0)\n",
    "#         )\n",
    "#         sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n",
    "#         phi = cosine * self.cos_m - sine * self.sin_m\n",
    "#         if self.easy_margin:\n",
    "#             phi = tf.where(cosine > 0, phi, cosine)\n",
    "#         else:\n",
    "#             phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n",
    "#         one_hot = tf.cast(\n",
    "#             tf.one_hot(y, depth=self.n_classes),\n",
    "#             dtype=cosine.dtype\n",
    "#         )\n",
    "#         if self.ls_eps > 0:\n",
    "#             one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n",
    "\n",
    "#         output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "#         output *= self.s\n",
    "#         return output\n",
    "\n",
    "\n",
    "# # Return tokens, masks and segments from a text array or series\n",
    "# def bert_encode(texts, tokenizer, max_len=512):\n",
    "#     all_tokens = []\n",
    "#     all_masks = []\n",
    "#     all_segments = []\n",
    "    \n",
    "#     for text in texts:\n",
    "#         text = tokenizer.tokenize(text)\n",
    "            \n",
    "#         text = text[:max_len-2]\n",
    "#         input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "#         pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "#         tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "#         tokens += [0] * pad_len\n",
    "#         pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "#         segment_ids = [0] * max_len\n",
    "        \n",
    "#         all_tokens.append(tokens)\n",
    "#         all_masks.append(pad_masks)\n",
    "#         all_segments.append(segment_ids)\n",
    "    \n",
    "#     return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n",
    "\n",
    "\n",
    "# # Function to get our text title embeddings using a pre-trained bert model\n",
    "# def get_text_embeddings(df, max_len = 70):\n",
    "#     embeds = []\n",
    "#     module_url = \"../input/shopee-external-models/bert_en_uncased_L-24_H-1024_A-16_1\"\n",
    "#     bert_layer = hub.KerasLayer(module_url, trainable = True)\n",
    "#     vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "#     do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "#     tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
    "# #     text = bert_encode(df['title'].values, tokenizer, max_len = max_len)\n",
    "#     text = bert_encode(df['title_clean'].values, tokenizer, max_len = max_len)\n",
    "    \n",
    "#     margin = ArcMarginProduct(\n",
    "#             n_classes = 11014, \n",
    "#             s = 30, \n",
    "#             m = 0.5, \n",
    "#             name='head/arc_margin', \n",
    "#             dtype='float32'\n",
    "#             )\n",
    "    \n",
    "#     input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "#     input_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "#     segment_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "#     label = tf.keras.layers.Input(shape = (), name = 'label')\n",
    "\n",
    "#     _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "#     clf_output = sequence_output[:, 0, :]\n",
    "#     x = margin([clf_output, label])\n",
    "#     output = tf.keras.layers.Softmax(dtype='float32')(x)\n",
    "#     model = tf.keras.models.Model(inputs = [input_word_ids, input_mask, segment_ids, label], outputs = [output])\n",
    "    \n",
    "#     # https://www.kaggle.com/ragnar123/bert-baseline\n",
    "#     model.load_weights('../input/checkpoints/Bert_123.h5')\n",
    "#     model = tf.keras.models.Model(inputs = model.input[0:3], outputs = model.layers[-4].output)\n",
    "#     chunk = 5000\n",
    "#     iterator = np.arange(np.ceil(len(df) / chunk))\n",
    "#     for j in iterator:\n",
    "#         a = int(j * chunk)\n",
    "#         b = int((j + 1) * chunk)\n",
    "#         text_chunk = ((text[0][a:b], text[1][a:b], text[2][a:b]))\n",
    "#         text_embeddings = model.predict(text_chunk, batch_size = 4)\n",
    "#         embeds.append(text_embeddings)\n",
    "#     del model\n",
    "#     text_embeddings = np.concatenate(embeds)\n",
    "#     print(f'Our text embeddings shape is {text_embeddings.shape}')\n",
    "#     del embeds\n",
    "#     gc.collect()\n",
    "#     return text_embeddings\n",
    "    \n",
    "\n",
    "# # Function to get 50 nearest neighbors of each image and apply a distance threshold to maximize cv\n",
    "# def get_neighbors(df, embeddings, KNN = 50, image = True):\n",
    "#     model = NearestNeighbors(n_neighbors = KNN)\n",
    "#     model.fit(embeddings)\n",
    "#     distances, indices = model.kneighbors(embeddings)\n",
    "    \n",
    "#     # Iterate through different thresholds to maximize cv, run this in interactive mode, then replace else clause with a solid threshold\n",
    "#     predictions = []\n",
    "#     for k in tqdm(range(embeddings.shape[0])):\n",
    "#         if image:\n",
    "#             idx = np.where(distances[k,] < 3.3)[0]\n",
    "#         else:\n",
    "#             idx = np.where(distances[k,] < 18.0)[0]\n",
    "#         ids = indices[k,idx]\n",
    "#         posting_ids = df['posting_id'].iloc[ids].values\n",
    "#         predictions.append(posting_ids)\n",
    "        \n",
    "#     del model, distances, indices\n",
    "#     gc.collect()\n",
    "#     return df, predictions\n",
    "\n",
    "\n",
    "# # df, df_cu, image_paths = read_dataset()\n",
    "# text_embeddings = get_text_embeddings(test)\n",
    "# print('text embeddings2 shape',text_embeddings.shape)\n",
    "# _ = gc.collect()\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# # Get neighbors for text_embeddings\n",
    "# ####################################\n",
    "# _knn = 50\n",
    "# if text_embeddings.shape[0] <= 3:\n",
    "#     _knn = 3\n",
    "# else:\n",
    "#     _knn = 50\n",
    "    \n",
    "# test, text_predictions = get_neighbors(test, text_embeddings, KNN=_knn, image=False)\n",
    "# test['text_embeddings2'] = text_predictions\n",
    "# del text_embeddings, text_predictions\n",
    "# gc.collect()\n",
    "\n",
    "\n",
    "################\n",
    "# other method\n",
    "################\n",
    "# preds = []\n",
    "# CHUNK = 1024\n",
    "\n",
    "# print('Finding similar titles...')\n",
    "# CTS = len(test) // CHUNK\n",
    "# if len(test) % CHUNK != 0: \n",
    "#     CTS += 1\n",
    "\n",
    "# for j in range( CTS ):\n",
    "    \n",
    "#     a = j*CHUNK\n",
    "#     b = (j+1)*CHUNK\n",
    "#     b = min(b,len(test))\n",
    "#     print('chunk',a,'to',b)\n",
    "    \n",
    "#     # COSINE SIMILARITY DISTANCE\n",
    "#     cts = cupy.matmul(text_embeddings, text_embeddings[a:b].T).T\n",
    "    \n",
    "#     for k in range(b-a):\n",
    "#         IDX = cupy.where(cts[k,]>0.7)[0]\n",
    "#         o = test.iloc[cupy.asnumpy(IDX)].posting_id.values\n",
    "#         preds.append(o.tolist())\n",
    "        \n",
    "# del text_embeddings\n",
    "# gc.collect()\n",
    "\n",
    "\n",
    "# test['text_embeddings2'] = preds\n",
    "# test.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Phash Feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = test.groupby('image_phash').posting_id.agg('unique').to_dict()\n",
    "test['phash'] = test.image_phash.map(tmp)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Image Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "dataset_root = '../input/shopee-product-matching'\n",
    "dataset_name = 'product'\n",
    "modelname = 'tf_efficientnet_b2_ns'\n",
    "test_batch_size = 8\n",
    "workers = 4\n",
    "no_cuda = False\n",
    "seed = 8\n",
    "\n",
    "MODELS = [\n",
    "#     '../input/checkpoints/(2021-04-03_172933)product_fold1_512x512_tf_efficientnet_b2_ns_acc(19.50891)_loss(6.97043)_checkpoint27.pth.tar',\n",
    "#     '../input/checkpoints/(2021-04-08_105923)product_fold3_192x192_dm_nfnet_f0_acc(59.31387)_loss(8.45285)_checkpoint28.pth.tar'\n",
    "   '../input/checkpoints/(2021-04-09_111137)product_fold3_260x260_tf_efficientnet_b2_ns_acc(0.00000)_loss(0.61341)_checkpoint29.pth.tar'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import TenCrop, FiveCrop, ToTensor, Lambda, Normalize\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations import ImageOnlyTransform\n",
    "from albumentations import SmallestMaxSize, HorizontalFlip, Compose, RandomCrop\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from timm.data.transforms_factory import transforms_imagenet_train\n",
    "from timm.data.random_erasing import RandomErasing\n",
    "\n",
    "# imagenet\n",
    "normalize = Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                      std=[0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "CROP_HEIGHT = 260\n",
    "CROP_WIDTH = 260\n",
    "\n",
    "\n",
    "def test_augmentation():\n",
    "    test_transform = [\n",
    "        A.Resize(CROP_HEIGHT, CROP_WIDTH),\n",
    "        A.Normalize(),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    "    return A.Compose(test_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision.models as torch_models\n",
    "\n",
    "import timm\n",
    "\n",
    "\n",
    "# https://www.kaggle.com/parthdhameliya77/pytorch-resnext50-32x4d-image-tfidf-inference\n",
    "class ArcMarginProduct(nn.Module):\n",
    "    def __init__(self, in_features, out_features, scale=30.0, margin=0.50, easy_margin=False, ls_eps=0.0):\n",
    "        super(ArcMarginProduct, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.scale = scale\n",
    "        self.margin = margin\n",
    "        self.ls_eps = ls_eps  # label smoothing\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(margin)\n",
    "        self.sin_m = math.sin(margin)\n",
    "        self.th = math.cos(math.pi - margin)\n",
    "        self.mm = math.sin(math.pi - margin) * margin\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        # --------------------------- cos(theta) & phi(theta) ---------------------------\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        # --------------------------- convert label to one-hot ---------------------------\n",
    "        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n",
    "        one_hot = torch.zeros(cosine.size(), device='cuda')\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n",
    "        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.scale\n",
    "\n",
    "        return output\n",
    "    \n",
    "\n",
    "class CosineSoftmaxModule(nn.Module):\n",
    "    def __init__(self, features_dim, nclass=11014):\n",
    "        super(CosineSoftmaxModule, self).__init__()\n",
    "        self.nclass = nclass\n",
    "\n",
    "        in_channels = features_dim\n",
    "\n",
    "        self.weights = torch.nn.Parameter(torch.randn(in_channels, self.nclass))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        self.scale = torch.nn.Parameter(F.softplus(torch.randn(())))\n",
    "        nn.init.xavier_uniform_(self.scale)\n",
    "        self.fc = nn.Linear(in_channels, in_channels)\n",
    "        self.dropout = nn.Dropout(p=0.1, inplace=False)\n",
    "\n",
    "    # pooler_output\n",
    "    def forward(self, x):\n",
    "        # x = x.view(-1, num_flat_features(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        features = x\n",
    "        # Features in rows, normalize axis 1.\n",
    "        features = F.normalize(features, p=2, dim=1, eps=1e-8)\n",
    "\n",
    "        # Mean vectors in colums, normalize axis 0.\n",
    "        weights_normed = F.normalize(self.weights, p=2, dim=0, eps=1e-8)\n",
    "        logits = self.scale.cuda() * torch.mm(features.cuda(), weights_normed.cuda())  # torch.matmul\n",
    "\n",
    "        return features, logits\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, model_name, use_fc=False, fc_dim=512, nclass=11014):\n",
    "        super(Model, self).__init__()\n",
    "        self.model_name = model_name\n",
    "        self.use_fc = use_fc\n",
    "        self.nclass = nclass\n",
    "\n",
    "        print('Building Model Backbone for {} model'.format(model_name))\n",
    "        model_names = timm.list_models(pretrained=True)\n",
    "#         pprint(model_names)\n",
    "        self.backbone = timm.create_model(model_name, pretrained=False, num_classes=nclass)\n",
    "        # Below code is used when if pretrained is False\n",
    "        pre_model = torch.load('../input/backbone/tf_efficientnet_b2_ns-00306e48.pth')           \n",
    "#         print(pre_model)\n",
    "#         # seresnext50_32x4d\n",
    "#         del pre_model['fc.weight']\n",
    "#         del pre_model['fc.bias']\n",
    "#         # tf_efficientnet_xx_ns\n",
    "        del pre_model['classifier.weight']\n",
    "        del pre_model['classifier.bias']\n",
    "        # dm_nfnet_fx\n",
    "#         del pre_model['head.fc.weight']\n",
    "#         del pre_model['head.fc.bias']\n",
    "        self.backbone.load_state_dict(pre_model, strict=False)\n",
    "\n",
    "        self.in_channels = 512  # resnet18, resnet34\n",
    "        if self.model_name in ['resnet18', 'resnet34', 'vgg16', 'vgg19']:\n",
    "            self.in_channels = 512\n",
    "        elif self.model_name in ['seresnext50_32x4d', 'resnext101_32x8d', 'resnext50_32x4d',\n",
    "                               'resnest50d', 'resnest101e', 'resnest200e', 'resnet50',\n",
    "                               'resnest269e', 'resnet101', 'resnet152']:\n",
    "            self.in_channels = 2048\n",
    "        elif self.model_name.startswith('tf_efficientnet_b0'):\n",
    "            self.in_channels = 1280\n",
    "        elif self.model_name.startswith('tf_efficientnet_b1'):\n",
    "            self.in_channels = 1280\n",
    "        elif self.model_name.startswith('tf_efficientnet_b2'):\n",
    "            self.in_channels = 1408\n",
    "        elif self.model_name.startswith('tf_efficientnet_b3'):\n",
    "            self.in_channels = 1536\n",
    "        elif self.model_name.startswith('tf_efficientnet_b4'):\n",
    "            self.in_channels = 1792\n",
    "        elif self.model_name.startswith('tf_efficientnet_b5'):\n",
    "            self.in_channels = 2048\n",
    "        elif self.model_name.startswith('dm_nfnet_f'):\n",
    "            self.in_channels = 3072\n",
    "\n",
    "        ##################\n",
    "        # cosine-softmax\n",
    "        ##################\n",
    "        # self.cosine_softmax = CosineSoftmaxModule(self.in_channels, nclass)\n",
    "\n",
    "        ##################\n",
    "        # ArcFace - https://www.kaggle.com/parthdhameliya77/pytorch-resnext50-32x4d-image-tfidf-inference\n",
    "        ##################\n",
    "        self.margin = ArcMarginProduct(fc_dim, self.nclass)\n",
    "        self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout = nn.Dropout(p=0.1, inplace=False)\n",
    "        self.fc = nn.Linear(self.in_channels, fc_dim)\n",
    "        self.bn = nn.BatchNorm1d(fc_dim)\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "        nn.init.constant_(self.fc.bias, 0)\n",
    "        nn.init.constant_(self.bn.weight, 1)\n",
    "        nn.init.constant_(self.bn.bias, 0)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        if self.model_name.startswith('tf_efficientnet'):\n",
    "            x = self.backbone.conv_stem(x)\n",
    "            x = self.backbone.bn1(x)\n",
    "            x = self.backbone.act1(x)\n",
    "            x = self.backbone.blocks(x)\n",
    "            x = self.backbone.conv_head(x)\n",
    "            x = self.backbone.bn2(x)\n",
    "            x = self.backbone.act2(x)\n",
    "#             x = self.backbone.global_pool(x)\n",
    "          \n",
    "        elif self.model_name.startswith('resnet') or \\\n",
    "                self.model_name.startswith('resnext') or \\\n",
    "                self.model_name.startswith('seresnext') or \\\n",
    "                self.model_name.startswith('resnest'):\n",
    "            x = self.backbone.conv1(x)\n",
    "            x = self.backbone.bn1(x)\n",
    "            x = self.backbone.act1(x)\n",
    "            x = self.backbone.maxpool(x)\n",
    "            x = self.backbone.layer1(x)\n",
    "            x = self.backbone.layer2(x)\n",
    "            x = self.backbone.layer3(x)\n",
    "            x = self.backbone.layer4(x)\n",
    "#             x = self.backbone.global_pool(x)\n",
    "            \n",
    "        elif self.model_name.startswith('dm_nfnet'):\n",
    "            x = self.backbone.stem(x)\n",
    "            x = self.backbone.stages(x)\n",
    "            x = self.backbone.final_conv(x)\n",
    "            x = self.backbone.final_act(x)\n",
    "#             x = self.backbone.head.global_pool(x)\n",
    "    \n",
    "        x = self.pooling(x).view(batch_size, -1)\n",
    "\n",
    "        ##################\n",
    "        # cosine-softmax\n",
    "        ##################\n",
    "        # return self.cosine_softmax(x)\n",
    "\n",
    "        ##################\n",
    "        # ArcFace - https://www.kaggle.com/parthdhameliya77/pytorch-resnext50-32x4d-image-tfidf-inference\n",
    "        ##################\n",
    "        if self.use_fc:\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc(x)\n",
    "            x = self.bn(x)\n",
    "\n",
    "        # logits when training\n",
    "#         return self.margin(x, labels)\n",
    "\n",
    "        # features when inference\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "\n",
    "\n",
    "class ProductTestDataset(data.Dataset):\n",
    "    def __init__(self, data_dir, csv, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.csv = csv\n",
    "        self.transform = transform\n",
    "\n",
    "        df = csv\n",
    "        self.images = df['image'].values.tolist()\n",
    "        self.posting_id = df['posting_id'].values.tolist()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = os.path.join(self.data_dir, 'test_images', self.images[index])\n",
    "        # print('#####: ', image_path)\n",
    "\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image=image)['image']\n",
    "\n",
    "        return image, self.posting_id[index], image_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "NUM_CLASS = 11014\n",
    "\n",
    "\n",
    "# global variable\n",
    "best_pred = 0.0\n",
    "acc_lst_train = []\n",
    "acc_lst_val = []\n",
    "\n",
    "\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "print(cuda)\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "model = Model(model_name=modelname, use_fc=False)\n",
    "# print('\\n-------------- model details --------------')\n",
    "# print(model)\n",
    "\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    model = nn.DataParallel(model)\n",
    "    \n",
    "\n",
    "test_df = pd.read_csv(\"../input/shopee-product-matching/test.csv\")\n",
    "\n",
    "CHUNK = 1024*4\n",
    "print('Computing image embeddings...')\n",
    "CTS = len(test_df)//CHUNK\n",
    "\n",
    "if len(test_df) % CHUNK != 0: \n",
    "    CTS += 1\n",
    "\n",
    "features_pool = []\n",
    "for resume in MODELS:    \n",
    "    if resume is not None:\n",
    "        if os.path.isfile(resume):\n",
    "            print(\"=> loading checkpoint '{}'\".format(resume))\n",
    "            checkpoint = torch.load(resume)\n",
    "            start_epoch = checkpoint['epoch'] + 1\n",
    "            best_pred = checkpoint['best_pred']\n",
    "            acc_lst_train = checkpoint['acc_lst_train']\n",
    "            acc_lst_val = checkpoint['acc_lst_val']\n",
    "            model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\".format(resume, checkpoint['epoch']))\n",
    "        else:\n",
    "            raise RuntimeError(\"=> no resume checkpoint found at '{}'\".format(resume))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    embeds = []    \n",
    "    for i,j in enumerate(range(CTS)):\n",
    "\n",
    "        a = j*CHUNK\n",
    "        b = (j+1)*CHUNK\n",
    "        b = min(b,len(test_df))\n",
    "        print('chunk',a,'to',b)\n",
    "\n",
    "        galleryset = ProductTestDataset(data_dir=dataset_root,\n",
    "                                        csv=test_df.iloc[a:b],\n",
    "                                        transform=test_augmentation())\n",
    "\n",
    "        gallery_loader = torch.utils.data.DataLoader(galleryset, batch_size=1, num_workers=workers)\n",
    "\n",
    "        gallery_features_list = []\n",
    "        tbar = tqdm(gallery_loader, desc='\\r')\n",
    "        for batch_idx, (data, pos_id, img_path) in enumerate(tbar):\n",
    "            if cuda:\n",
    "                data = data.cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # num_tta = 8\n",
    "#                 x = torch.stack([data, data.flip(-1), data.flip(-2), data.flip(-1,-2),\n",
    "#                                  data.transpose(-1,-2), data.transpose(-1,-2).flip(-1),\n",
    "#                                  data.transpose(-1,-2).flip(-2), data.transpose(-1,-2).flip(-1,-2)], 0)\n",
    "                # num_tta = 6\n",
    "                x = torch.stack([data, data.flip(-1), data.flip(-2),\n",
    "                                 data.transpose(-1,-2), data.transpose(-1,-2).flip(-1),\n",
    "                                 data.transpose(-1,-2).flip(-2)], 0)\n",
    "\n",
    "#                 x = torch.stack([data], 0)\n",
    "                x = x.view(-1, 3, CROP_HEIGHT, CROP_WIDTH)\n",
    "#                 #################\n",
    "#                 # cosine-softmax\n",
    "#                 #################\n",
    "#                 features, _ = model(x)\n",
    "                ##########\n",
    "                # ArcFace\n",
    "                ##########\n",
    "                features = model(x)\n",
    "\n",
    "#                 features = features.view(1, 5, -1).mean(1)\n",
    "                features = features.view(1, 6, -1).max(1)[0] # values\n",
    "                gallery_features_list.append(features.cpu().numpy())\n",
    "\n",
    "        embeds.extend(gallery_features_list)   \n",
    "    \n",
    "    features_pool.append(np.concatenate(embeds))\n",
    "    _ = gc.collect()\n",
    "\n",
    "# -------------\n",
    "# max pooling.\n",
    "# -------------\n",
    "gallery_features = features_pool[0]\n",
    "for i in range(1, len(features_pool)):\n",
    "    gallery_features = np.maximum(gallery_features, features_pool[i])\n",
    "\n",
    "\n",
    "print('image embeddings shape', gallery_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import cudf, cuml\n",
    "# import cupy\n",
    "\n",
    "# image_embeddings = cupy.array(gallery_features)\n",
    "\n",
    "# preds = []\n",
    "# CHUNK = 1024*4\n",
    "\n",
    "# print('Finding similar images...')\n",
    "# CTS = len(image_embeddings) // CHUNK\n",
    "# if len(image_embeddings) % CHUNK != 0: \n",
    "#     CTS += 1\n",
    "\n",
    "# for j in range( CTS ):\n",
    "#     a = j*CHUNK\n",
    "#     b = (j+1)*CHUNK\n",
    "#     b = min(b,len(image_embeddings))\n",
    "#     print('chunk',a,'to',b)\n",
    "    \n",
    "#     cts = cupy.matmul(image_embeddings, image_embeddings[a:b].T).T\n",
    "    \n",
    "#     for k in range(b-a):\n",
    "#         print('cts[k,]: ', cts[k,])\n",
    "#         IDX = cupy.where(cts[k,]>0.7)[0]\n",
    "#         print('IDX: ', IDX)\n",
    "#         o = test_df.iloc[cupy.asnumpy(IDX)].posting_id.values\n",
    "# #         preds.append(' '.join(o.tolist()))\n",
    "#         preds.append(o.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(test_df) > 3:\n",
    "    KNN = 50\n",
    "else : \n",
    "    KNN = 3\n",
    "\n",
    "model = NearestNeighbors(n_neighbors = KNN)\n",
    "model.fit(gallery_features)\n",
    "distances, indices = model.kneighbors(gallery_features)\n",
    "print('distances: ', distances)\n",
    "print('indices: ', indices)\n",
    "\n",
    "preds = []\n",
    "for k in tqdm(range(gallery_features.shape[0])):\n",
    "    idx = np.where(distances[k,] < 4.8)[0]\n",
    "    ids = indices[k,idx]\n",
    "    posting_ids = test_df['posting_id'].iloc[ids].values\n",
    "    preds.append(posting_ids)\n",
    "\n",
    "del model, distances, indices\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['image_embeddings'] = preds\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_for_sub(row):\n",
    "#     x = np.concatenate([row.text_embeddings, row.text_embeddings2, row.phash, row.image_embeddings])\n",
    "    x = np.concatenate([row.image_embeddings, row.text_embeddings])\n",
    "#     print('x: ', x)\n",
    "#     x = row.image_embeddings\n",
    "#     print('x: ', x)\n",
    "#     return ' '.join( np.unique(x) )\n",
    "\n",
    "    # https://stackoverflow.com/questions/12926898/numpy-unique-without-sort\n",
    "    indexes = np.unique(x, return_index=True)[1]\n",
    "    return ' '.join([x[index] for index in sorted(indexes)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df = pd.read_csv(\"../input/shopee-product-matching/sample_submission.csv\")\n",
    "\n",
    "submit_df['matches'] = test.apply(combine_for_sub, axis=1)\n",
    "submit_df[['posting_id', 'matches']].to_csv('submission.csv', index=False)\n",
    "\n",
    "sub = pd.read_csv('submission.csv')\n",
    "sub.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
